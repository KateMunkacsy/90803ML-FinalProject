{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c3dca8-fec2-4554-ac07-3033e78172c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 6A7C-0C86\n",
      "\n",
      " Directory of C:\\Users\\lizie\\OneDrive\\Desktop\\CMU\\25spring\\ML\\group project\\90803ML-FinalProject\n",
      "\n",
      "04/15/2025  12:27 AM    <DIR>          .\n",
      "04/14/2025  11:45 PM    <DIR>          ..\n",
      "04/15/2025  12:25 AM    <DIR>          .ipynb_checkpoints\n",
      "04/14/2025  11:45 PM    <DIR>          Data\n",
      "04/14/2025  11:50 PM            47,719 Data prep and baseline models.ipynb\n",
      "04/14/2025  11:45 PM            26,427 EDA.ipynb\n",
      "04/15/2025  12:27 AM             1,395 feature generation and model.ipynb\n",
      "04/15/2025  12:17 AM            54,752 initial_data_import.ipynb\n",
      "04/14/2025  11:45 PM                92 README.md\n",
      "               5 File(s)        130,385 bytes\n",
      "               4 Dir(s)  741,817,737,216 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f281ea1-b1c6-4512-bbd5-c079b91c20f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\lizie\\OneDrive\\Desktop\\CMU\\25spring\\ML\\group project\\90803ML-FinalProject\\Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc3857c-a6e1-434e-98b0-8612b7402e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39afdacb-0bc2-4c08-882e-56c6fd6ed81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following code, I'm trying XGBoost and LightGBM model for raw data along with baseline models and see which performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ca311f-1188-4d98-b018-675623d2567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Mean) | Mean RMSE: 1393.617 | Std: 41.610\n",
      "Linear Regression | Mean RMSE: 692.996 | Std: 22.712\n",
      "Random Forest | Mean RMSE: 236.592 | Std: 12.999\n",
      "XGBoost | Mean RMSE: 414.290 | Std: 10.813\n",
      "LightGBM | Mean RMSE: 264.560 | Std: 13.764\n"
     ]
    }
   ],
   "source": [
    "ridership = pd.read_csv(\"ridership.csv\")\n",
    "\n",
    "# Same X and y\n",
    "X = ridership.drop(['avg_riders', 'route_full_name'], axis=1)\n",
    "y = ridership['avg_riders']\n",
    "\n",
    "# Drop sparse columns \n",
    "sparse_columns = list(X.columns[X.nunique() / len(X) * 100 < 0.01])\n",
    "X = X.drop(sparse_columns, axis=1)\n",
    "\n",
    "# Identify column types\n",
    "int_cols = X.select_dtypes(include='int').columns.tolist()\n",
    "float_cols = X.select_dtypes(include='float').columns.tolist()\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Preprocessing\n",
    "numeric_preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_preprocessor, int_cols + float_cols),\n",
    "    ('cat', categorical_preprocessor, cat_cols)\n",
    "])\n",
    "\n",
    "# Define XGBoost and LightGBM pipelines\n",
    "models = {\n",
    "    \"Baseline (Mean)\": DummyRegressor(strategy='mean'),\n",
    "    \"Linear Regression\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    \"Random Forest\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(n_estimators=25, random_state=47, n_jobs=-1))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    rmse_scores = -scores\n",
    "    print(f\"{name} | Mean RMSE: {rmse_scores.mean():.3f} | Std: {rmse_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038e961-3676-4719-8ed0-8e8d4d8e2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like Random Forest behaves best, but we can still explore LightGBM further as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a40bd-6b9d-47cd-9980-eaa8027d3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then explore feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3d1ada2-c75a-47a8-9ca7-8b7437dbc2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse columns: []\n",
      "Int columns: ['year_month', 'day_count']\n",
      "Float columns: ['total_precip', 'avg_temp']\n",
      "Cat columns: ['route', 'ridership_route_code', 'route_full_name', 'current_garage', 'mode', 'month_start', 'day_type']\n",
      "Baseline (Mean) | Mean RMSE: 1393.617 | Std: 41.610\n",
      "Linear Regression | Mean RMSE: 694.691 | Std: 23.233\n",
      "Random Forest | Mean RMSE: 238.076 | Std: 15.704\n",
      "XGBoost | Mean RMSE: 198.764 | Std: 16.186\n",
      "LightGBM | Mean RMSE: 187.354 | Std: 9.442\n"
     ]
    }
   ],
   "source": [
    "ridership = pd.read_csv(\"ridership.csv\")\n",
    "\n",
    "# Same X and y\n",
    "# X = ridership.drop(['_id', 'route', 'avg_riders', 'route_full_name', 'year_month'], axis=1)\n",
    "X = ridership.drop(['_id','avg_riders'], axis=1)\n",
    "y = ridership['avg_riders']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Drop sparse columns \n",
    "sparse_columns = list(X.columns[X.nunique() / len(X) * 100 < 0.01])\n",
    "print(f\"Sparse columns: {sparse_columns}\")\n",
    "\n",
    "# Identify column types\n",
    "int_cols = X.select_dtypes(include='int').columns.tolist()\n",
    "float_cols = X.select_dtypes(include='float').columns.tolist()\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "print(f\"Int columns: {int_cols}\")\n",
    "print(f\"Float columns: {float_cols}\")\n",
    "print(f\"Cat columns: {cat_cols}\")\n",
    "\n",
    "# Preprocessing\n",
    "numeric_preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_preprocessor, int_cols + float_cols),\n",
    "    ('cat', categorical_preprocessor, cat_cols)\n",
    "])\n",
    "\n",
    "\n",
    "# Define XGBoost and LightGBM pipelines\n",
    "models = {\n",
    "    \"Baseline (Mean)\": DummyRegressor(strategy='mean'),\n",
    "    \"Linear Regression\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    \"Random Forest\": Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(n_estimators=25, random_state=47, n_jobs=8))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(n_estimators=1000, max_depth=10, learning_rate=0.1, random_state=42))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LGBMRegressor(n_estimators=1000, learning_rate=0.1, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=8)\n",
    "    rmse_scores = -scores\n",
    "    print(f\"{name} | Mean RMSE: {rmse_scores.mean():.3f} | Std: {rmse_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5774221-6346-4eb6-aef7-5161242ca87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1104\n",
      "[LightGBM] [Info] Number of data points in the train set: 22317, number of used features: 420\n",
      "[LightGBM] [Info] Start training from score 1189.182466\n",
      "                                     feature  importance\n",
      "0                            num__year_month        7793\n",
      "3                              num__avg_temp        3270\n",
      "1                             num__day_count        2802\n",
      "2                          num__total_precip        2751\n",
      "442                       cat__day_type_SUN.         839\n",
      "441                       cat__day_type_SAT.         806\n",
      "338         cat__current_garage_East Liberty         716\n",
      "342         cat__current_garage_West Mifflin         617\n",
      "340                 cat__current_garage_Ross         501\n",
      "337              cat__current_garage_Collier         456\n",
      "105                           cat__route_RED         318\n",
      "437              cat__month_start_2024-07-01         187\n",
      "80                           cat__route_BLUE         173\n",
      "60                            cat__route_71B         170\n",
      "51                            cat__route_61D         147\n",
      "59                            cat__route_71A         146\n",
      "48                            cat__route_61A         145\n",
      "61                            cat__route_71C         140\n",
      "341  cat__current_garage_South Hills Village         134\n",
      "49                            cat__route_61B         134\n"
     ]
    }
   ],
   "source": [
    "# identify important features\n",
    "model.fit(X, y)\n",
    "importances = model.named_steps['regressor'].feature_importances_\n",
    "feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "feat_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "print(feat_df.sort_values(by='importance', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e61edc-33aa-4760-b433-7ea84fa5d0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
